Bayes Theorem
  P(A|B) = (P(B|A)P(A))/P(B)
Conditional
  p(A|B) = p(A ∩ B)/p(B)

Experiment:
  procedure that results in one out of a number of possible outcomes
Sample Space:
  Set of all possible outcomes
Event:
  Subset of the sample space
Outcome
    possible happening or occurrence

Discrete Probability
    sample space is finite or countably infinite set
    probability mass functions
Countably Infinite:
    if there is a one-to-one (injective) correspondence between the elements of the set and the integers
    Examples:
      binary strings, rational numbers, ordered pairs of integers
Uncountably Infinite
    set of Real numbers (infinite amount of numbers between 0 and 1)
    continuous probability
    get number within a range
      probability density functions

See Probability Distribution.md

Uniform Distribution
  Every outcome has the same probability
  p(s) = 1 / |S|     cardinality of S, or number of elements
  p(E) = |E|/|S|

If two events are disjoint, then
  p(E1 ∪ E2) = p(E1) + p(E2)

If Not Disjoint
  p(E1 ∪ E2) = p(E1) + p(E2) - p(E1 ∩ E2)

Complement
  p(E) + p(E') = 1

Conditional
  probability of E given F
  p(E|F) = p(E ∩ F)/ p(F)
  if  distribution is uniform, then:
    p(E|F) = |E∩F|/|F|

Conditional Complement
  p(E|F) + p(E'|F) = 1

Independent Events
  One of these conditions must be true:
  p(E|F) = p(E∩F)/p(F) = p(E)
  p(E∩F) = p(E) * p(F)
  p(F|E) = p(E∩F)/p(E) = p(F)

Mutual Indenpendence
  IFF p(A∩B∩C) = p(A) * p(B) * p(C)

Bayes Theorem
  p(F|X) = p(X|F)p(F)/[p(X|F)p(F) + p(X|F')p(F')] = p(X|F)p(F)/p(X)

**NOTE**
The probability that if a person has a disease they test positive is p(T|B)
BUT
The probablility that if a person tests positive has the disease is p(B|T) and is Bayes Theorem
  the probability is usually much lower

Random Variable
  A random variable X is a function from the sample space S of an experiment to the real nmbers.  X(S) denotes the range of
the function X
Random Variable and Probabilities
  If X is a random variable defined on the sample space S and r is a real number, the X = r is an event.
  The event X = r consists of all outcomes s in the sample space such that X(s)=r.
  p(X=r) is the sum of p(s) for all s such that X(s) = r. 
Distribution over a Random Variable
  Set of all pairs (r, p(X=r)) such that r ∈ X(S)
  Σ of all p(X=r) must equal 1
  each pair in set consists of value of X(S) (which is r) and p(X=r)
  Example: {(2,1/36), (3,1/18),...,(12,1/36)}

Expected Value of a random variable
  E[X] = sum for all s:  X(s)*p(s)
Alternative
  E[X] = sum for all r:  r * p(X=r)

Linearity of Expectations
  E[X+Y] = E[X] + E[Y]
  E[cX] = cE[X]
  and applies to more than two variables E[X+Y+Z+...] = E[X] + E[Y] + E[Z] + ...

Bernoulli Trial
  Experiment with two outcomes: success or failure
Bernoulli Process:
  repeated trials with mutually independent outcomes and same probability of success or failure
Success: p
Failure: q or 1-p
Bernoulli Trial Probabilities:
  probability of exactly k successes in sequence of n trials
  nCk * p^k * q^(n-k)
E[k] = n * p

Binomial Distribution
  distribution over random variable of number of successes in sequence of Bernoulli trials
  probability b is y-axis, #successes k is x-axis
  b(k;n,p) = nCk * p^k * q^(n-k)
